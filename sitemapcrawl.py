#!/usr/bin/env python3
"""
Keywordâ€¯Sitemapâ€¯Crawlerâ€¯â€”â€¯UltimateÂ TurboÂ Edition ğŸš€
=================================================
* **Twoâ€‘phase scan**Â â€“Â Piceapp first (massively async) â†’ Jamku fallback (rateâ€‘limited but parallel).
* **Rootâ€‘aware HSâ€‘code keyword matching** with ultraâ€‘fast *preâ€‘check* to skip regex on nonâ€‘candidates.
* **Maxâ€‘speed I/O**: `aiohttp` w/ HTTPâ€‘2, keepâ€‘alive, gzip, DNS cache; configurable concurrency & rate.
* **Smart retries** with exponential backâ€‘off and separate Jamku retry logic.
* **Streamlit UI**: minimal redraws, autoâ€‘refresh to prevent idle timeout, live checkpoints, largeâ€‘data safeguards.
* **Resumable** â€“ writes partial CSV every *n* batches so you never lose progress.

Â©Â 2025Â â€”Â Designed for 1â€¯M+ pages. MITÂ License.
"""

from __future__ import annotations

import asyncio
import gzip
import logging
import re
import time
from datetime import datetime
from io import BytesIO
from pathlib import Path
from typing import Dict, List, Sequence, Tuple

import aiohttp
import async_timeout
import pandas as pd
import streamlit as st
from bs4 import BeautifulSoup
from lxml import etree
from more_itertools import chunked
import altair as alt
import ujson as json  # ultraâ€‘fast JSON for checkpoints

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Config â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
DEFAULT_TIMEOUT: int = 20              # seconds per request
DEFAULT_CONCURRENCY: int = 100         # max parallel fetches for Piceapp phase
DEFAULT_BATCH_SIZE: int = 1_000        # pages per gather() batch
REGEX_CHUNK_SIZE: int = 1_000          # tokens per compiled regex (avoids "tooâ€‘large" regex)
DNS_CACHE_TTL: int = 300               # seconds
GSTIN_RE = re.compile(r"[0-9]{2}[A-Z0-9]{10}[0-9A-Z]{3}", re.I)
CHECKPOINT_EVERY: int = 10             # write partial CSV every *n* batches
SAMPLE_PRECHECK: int = 20              # how many keywords to use for quick strâ€‘in preâ€‘check

logging.basicConfig(level=logging.INFO, format="[%(levelname)s] %(message)s")
LOG = logging.getLogger("kws-crawler")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Keyword helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def _validate_keywords(raw: Sequence[str]) -> List[str]:
    """Drop empty/tooâ€‘short tokens & weird chars."""
    return [k.strip() for k in raw if re.fullmatch(r"[\w\- ]{2,}", k.strip())]


@st.cache_data(show_spinner=False)
def build_keyword_structures(raw: Sequence[str]) -> Tuple[List[str], Dict[str, str]]:
    """Return flat regex tokens + rootâ€‘lookup map (cached)."""
    cleaned = sorted({k for k in _validate_keywords(raw)})
    roots = {k for k in cleaned if k.isdigit() and len(k) <= 4}  # 4â€‘digit masters

    flat: List[str] = []
    mapping: Dict[str, str] = {}

    for kw in cleaned:
        if kw.isdigit() and len(kw) == 4:
            mapping[kw] = kw
            flat.append(kw)
            family_regex = fr"\b{kw}\d{{4}}\b"  # children
            mapping[family_regex] = kw
            flat.append(family_regex)
        else:
            root = kw[:4] if kw[:4] in roots else kw
            mapping[kw] = root
            flat.append(kw)
    return flat, mapping


def compile_patterns(tokens: Sequence[str]) -> List[re.Pattern]:
    """Chunkâ€‘compile huge regex list to avoid re.error."""
    return [
        re.compile("|".join(tokens[i : i + REGEX_CHUNK_SIZE]), re.I)
        for i in range(0, len(tokens), REGEX_CHUNK_SIZE)
    ]

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ HTTP fetch with retries â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

async def fetch(session: aiohttp.ClientSession, url: str, retries: int = 3, backoff: float = 1.7) -> bytes:
    for attempt in range(1, retries + 1):
        try:
            async with async_timeout.timeout(DEFAULT_TIMEOUT):
                async with session.get(url, ssl=False) as resp:
                    resp.raise_for_status()
                    return await resp.read()
        except Exception as exc:
            LOG.warning("[Retry %s/%s] %s â€“ %s", attempt, retries, url, exc)
            if attempt == retries:
                raise
            await asyncio.sleep(backoff ** attempt)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Sitemap discovery â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

async def _locs_from_sitemap(session: aiohttp.ClientSession, url: str) -> List[str]:
    try:
        raw = await fetch(session, url)
        if url.endswith(".gz"):
            raw = gzip.decompress(raw)
        tree = etree.parse(BytesIO(raw))
        root_name = etree.QName(tree.getroot()).localname
        return [loc.text.strip() for loc in tree.findall(".//{*}loc")]
        if root_name in {"urlset", "sitemapindex"} else []
    except Exception as exc:
        LOG.warning("Parse failed %s â€“ %s", url, exc)
        return []

async def discover_urls(session: aiohttp.ClientSession, sitemaps: List[str]) -> List[str]:
    queue, seen, pages = list(dict.fromkeys(sitemaps)), set(), []
    prog = st.empty()
    done = 0
    while queue:
        sm = queue.pop(0)
        if sm in seen:
            continue
        seen.add(sm)
        locs = await _locs_from_sitemap(session, sm)
        if locs and locs[0].lower().endswith((".xml", ".gz")):
            queue.extend(u for u in locs if u not in seen)  # recurse
        else:
            pages.extend(locs)
        done += 1
        if done % 5 == 0:
            prog.progress(done / (done + len(queue) + 1e-9), text="ğŸ” Discovering URLs â€¦")
    prog.empty()
    return list(dict.fromkeys(pages))

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ GSTIN URL helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def to_jamku_url(pice_url: str) -> str | None:
    if m := GSTIN_RE.search(pice_url):
        return f"https://gst.jamku.app/gstin/{m.group(0).lower()}"
    return None

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Page scan â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

async def _scan_page(
    session: aiohttp.ClientSession,
    url: str,
    patterns: List[re.Pattern],
    simple_keywords: Sequence[str],
    sem: asyncio.Semaphore,
) -> Tuple[str, str, str] | None:
    """Return (url, matched_keyword, title) or None on noâ€‘match; raises for HTTP errors."""
    async with sem:
        html_bytes = await fetch(session, url)
        html = html_bytes.decode("utf-8", "ignore")

        # ğŸ” Ultraâ€‘fast preâ€‘check to skip regex on obvious nonâ€‘matches
        if not any(kw in html for kw in simple_keywords):
            return None

        match = next((m for p in patterns if (m := p.search(html))), None)
        if match:
            # ğŸŒŸ Minimize HTML parsing â€“ only when match found
            soup = BeautifulSoup(html, "html.parser")
            title = (soup.title.string or "").strip() if soup.title else ""
            return url, match.group(0), title
        return None

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Jamku scan helper â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

async def _scan_jamku_wrapper(
    session: aiohttp.ClientSession,
    url: str,
    patterns: List[re.Pattern],
    simple_keywords: Sequence[str],
    sem: asyncio.Semaphore,
    rate_delay: float,
):
    async with sem:  # respect parallel limit
        try:
            res = await _scan_page(session, url, patterns, simple_keywords, sem=asyncio.Semaphore(1))
            return res
        finally:
            await asyncio.sleep(rate_delay)  # global rateâ€‘limit

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Main crawl routine â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

async def crawl_and_retry(
    sitemaps: List[str],
    raw_keywords: List[str],
    concurrency: int,
    batch_size: int,
    jamku_rate: int,
) -> pd.DataFrame:
    """Phaseâ€‘1 Piceapp scan â†’ parallel, rateâ€‘limited Jamku retry."""

    tokens, root_map = build_keyword_structures(tuple(raw_keywords))  # cached
    patterns = compile_patterns(tokens)
    simple_keywords = tokens[:SAMPLE_PRECHECK]

    # HTTP client
    conn = aiohttp.TCPConnector(
        limit=concurrency,
        ttl_dns_cache=DNS_CACHE_TTL,
        force_close=False,
        enable_http2=True,
    )
    timeout = aiohttp.ClientTimeout(total=DEFAULT_TIMEOUT)
    headers = {"Accept-Encoding": "gzip"}

    results: List[Tuple[str, str, str]] = []
    failed_pice_urls: List[str] = []

    async with aiohttp.ClientSession(connector=conn, timeout=timeout, headers=headers) as session:
        # 1ï¸âƒ£ Discover URLs
        base_pages = await discover_urls(session, sitemaps)
        st.info(f"ğŸŒ Total Piceapp pages discovered: {len(base_pages):,}")

        # 2ï¸âƒ£ Highâ€‘speed Piceapp scan
        sem = asyncio.Semaphore(concurrency)
        scanned = 0
        prog = st.empty()
        start = time.perf_counter()

        for batch_idx, batch in enumerate(chunked(base_pages, batch_size), start=1):
            tasks = [
                _scan_page(session, u, patterns, simple_keywords, sem) for u in batch
            ]
            for res in await asyncio.gather(*tasks, return_exceptions=True):
                scanned += 1
                if isinstance(res, tuple):
                    results.append(res)
                elif isinstance(res, Exception):
                    failed_pice_urls.append(res.args[0] if res.args else "")
            # update UI sparingly
            if batch_idx % 1 == 0:
                prog.progress(
                    min(1.0, scanned / len(base_pages)),
                    text=f"âš¡ Scanning Piceapp â€¦ {scanned}/{len(base_pages):,}",
                )
            # checkpoint
            if batch_idx % CHECKPOINT_EVERY == 0:
                _checkpoint(results)
        prog.empty()
        st.success(f"âœ… Piceapp phase completed in {time.perf_counter() - start:.1f}s")

        # 3ï¸âƒ£ Prepare Jamku list
        jamku_urls = [u for u in {to_jamku_url(p) for p in failed_pice_urls} if u]
        if not jamku_urls:
            st.info("ğŸ‰ No failed URLs to retry on Jamku.")
            return _to_df(results, root_map)

        st.info(f"â™»ï¸ Retrying {len(jamku_urls):,} failed URLs via Jamku ({jamku_rate}/sec max)")

        # 4ï¸âƒ£ Parallel rateâ€‘limited Jamku scan
        sem_jamku = asyncio.Semaphore(jamku_rate)
        rate_delay = 1 / jamku_rate
        prog_j = st.empty()

        jamku_tasks = [
            _scan_jamku_wrapper(session, u, patterns, simple_keywords, sem_jamku, rate_delay)
            for u in jamku_urls
        ]
        completed = 0
        for f in asyncio.as_completed(jamku_tasks):
            res = await f
            completed += 1
            if isinstance(res, tuple):
                results.append(res)
            if completed % 10 == 0:
                prog_j.progress(completed / len(jamku_urls), text="ğŸ”„ Scanning Jamku â€¦")
        prog_j.empty()

    return _to_df(results, root_map)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def _checkpoint(records: List[Tuple[str, str, str]]):
    """Write incremental checkpoint to disk using fast ujson."""
    ts = datetime.now().strftime("%Y%m%d_%H%M%S")
    path = Path(f"checkpoint_{ts}.json")
    with path.open("w", encoding="utf-8") as fp:
        json.dump(records[-10_000:], fp)  # store latest slice to keep file size small
    LOG.info("ğŸ’¾ Checkpoint saved â†’ %s", path)


def _to_df(records: List[Tuple[str, str, str]], root_map: Dict[str, str]) -> pd.DataFrame:
    df = pd.DataFrame(records, columns=["url", "matched_keyword", "page_title"])
    df["matched_root"] = df["matched_keyword"].map(root_map).fillna(df["matched_keyword"])
    return df[["url", "matched_keyword", "matched_root", "page_title"]]

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Streamlit UI â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

st.set_page_config(page_title="Keyword Sitemap Crawler", layout="wide")

st_autorefresh = st.experimental_rerun  # futureâ€‘proof; simple keepâ€‘alive

st.title("ğŸ” KeywordÂ SitemapÂ CrawlerÂ â€” UltimateÂ Turbo ğŸš€")

with st.form("crawl_form"):
    sitemaps_input = st.text_area("Sitemap URL(s) (one per line)", height=120)
    kw_file = st.file_uploader("Keyword list (TXT)", type=["txt"])
    col1, col2, col3 = st.columns(3)
    concurrency = col1.slider("Concurrency (Piceapp phase)", 20, 800, DEFAULT_CONCURRENCY, 20)
    batch_size = col2.slider("Batch size", 200, 5_000, DEFAULT_BATCH_SIZE, 200)
    jamku_rate = col3.slider("Jamku requests / sec", 1, 10, 3)
    submitted = st.form_submit_button("ğŸš€ Run Crawl")

if submitted:
    if not sitemaps_input or not kw_file:
        st.error("âŒ Provide both sitemap URLs and a keyword list.")
        st.stop()

    sitemap_list = [u.strip() for u in sitemaps_input.splitlines() if u.strip()]
    raw_kw = kw_file.read().decode("utf-8", "ignore").splitlines()
    st.write(f"ğŸ“„ {len(raw_kw):,} keywords loaded.")

    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)
    try:
        df_out = loop.run_until_complete(
            crawl_and_retry(sitemap_list, raw_kw, concurrency, batch_size, jamku_rate)
        )
    finally:
        loop.run_until_complete(loop.shutdown_asyncgens())
        loop.close()

    # â€” Results
    if df_out.empty:
        st.warning("âš ï¸ No keyword matches found.")
    else:
        st.dataframe(df_out, use_container_width=True)
        csv_bytes = df_out.to_csv(index=False).encode("utf-8")
        st.download_button("ğŸ“¥ Download CSV", csv_bytes, "keyword_matches.csv", "text/csv")

        # Offload chart for huge outputs
        if len(df_out) < 20_000:
            chart_data = (
                df_out["matched_root"].value_counts().reset_index(names=["matched_root", "count"])
            )
            bar = (
                alt.Chart(chart_data)
                .mark_bar()
                .encode(x="matched_root:N", y="count:Q", tooltip=["matched_root", "count"])
                .properties(title="Match count by root", height=400)
            )
            st.altair_chart(bar, use_container_width=True)
        else:
            st.info("ğŸ“Š Chart skipped due to large result set (â‰¥20K rows).")
