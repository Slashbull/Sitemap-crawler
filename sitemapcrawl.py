#!/usr/bin/env python3
"""
Keyword‚ÄØSitemap‚ÄØCrawler‚ÄØ‚Äî‚ÄØUltimate¬†Turbo¬†Edition üöÄ
=================================================
* **Two‚Äëphase scan**¬†‚Äì¬†Piceapp first (massively async) ‚Üí Jamku fallback (rate‚Äëlimited but parallel).
* **Root‚Äëaware HS‚Äëcode keyword matching** with ultra‚Äëfast *pre‚Äëcheck* to skip regex on non‚Äëcandidates.
* **Max‚Äëspeed I/O**: `aiohttp` w/ HTTP‚Äë2, keep‚Äëalive, gzip, DNS cache; configurable concurrency & rate.
* **Smart retries** with exponential back‚Äëoff and separate Jamku retry logic.
* **Streamlit UI**: minimal redraws, auto‚Äërefresh to prevent idle timeout, live checkpoints, large‚Äëdata safeguards.
* **Resumable** ‚Äì writes partial CSV every *n* batches so you never lose progress.

¬©¬†2025¬†‚Äî¬†Designed for 1‚ÄØM+ pages. MIT¬†License.
"""

from __future__ import annotations

import asyncio
import gzip
import logging
import re
import time
from datetime import datetime
from io import BytesIO
from pathlib import Path
from typing import Dict, List, Sequence, Tuple

import aiohttp
import async_timeout
import pandas as pd
import streamlit as st
from bs4 import BeautifulSoup
from lxml import etree
from more_itertools import chunked
import altair as alt
import ujson as json  # ultra‚Äëfast JSON for checkpoints

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Config ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
DEFAULT_TIMEOUT: int = 20              # seconds per request
DEFAULT_CONCURRENCY: int = 100         # max parallel fetches for Piceapp phase
DEFAULT_BATCH_SIZE: int = 1_000        # pages per gather() batch
REGEX_CHUNK_SIZE: int = 1_000          # tokens per compiled regex (avoids "too‚Äëlarge" regex)
DNS_CACHE_TTL: int = 300               # seconds
GSTIN_RE = re.compile(r"[0-9]{2}[A-Z0-9]{10}[0-9A-Z]{3}", re.I)
CHECKPOINT_EVERY: int = 10             # write partial CSV every *n* batches
SAMPLE_PRECHECK: int = 20              # how many keywords to use for quick str‚Äëin pre‚Äëcheck

logging.basicConfig(level=logging.INFO, format="[%(levelname)s] %(message)s")
LOG = logging.getLogger("kws-crawler")

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Keyword helpers ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def _validate_keywords(raw: Sequence[str]) -> List[str]:
    """Drop empty/too‚Äëshort tokens & weird chars."""
    return [k.strip() for k in raw if re.fullmatch(r"[\w\- ]{2,}", k.strip())]


@st.cache_data(show_spinner=False)
def build_keyword_structures(raw: Sequence[str]) -> Tuple[List[str], Dict[str, str]]:
    """Return flat regex tokens + root‚Äëlookup map (cached)."""
    cleaned = sorted({k for k in _validate_keywords(raw)})
    roots = {k for k in cleaned if k.isdigit() and len(k) <= 4}  # 4‚Äëdigit masters

    flat: List[str] = []
    mapping: Dict[str, str] = {}

    for kw in cleaned:
        if kw.isdigit() and len(kw) == 4:
            mapping[kw] = kw
            flat.append(kw)
            family_regex = fr"\b{kw}\d{{4}}\b"  # children
            mapping[family_regex] = kw
            flat.append(family_regex)
        else:
            root = kw[:4] if kw[:4] in roots else kw
            mapping[kw] = root
            flat.append(kw)
    return flat, mapping


def compile_patterns(tokens: Sequence[str]) -> List[re.Pattern]:
    """Chunk‚Äëcompile huge regex list to avoid re.error."""
    return [
        re.compile("|".join(tokens[i : i + REGEX_CHUNK_SIZE]), re.I)
        for i in range(0, len(tokens), REGEX_CHUNK_SIZE)
    ]

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ HTTP fetch with retries ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

async def fetch(session: aiohttp.ClientSession, url: str, retries: int = 3, backoff: float = 1.7) -> bytes:
    for attempt in range(1, retries + 1):
        try:
            async with async_timeout.timeout(DEFAULT_TIMEOUT):
                async with session.get(url, ssl=False) as resp:
                    resp.raise_for_status()
                    return await resp.read()
        except Exception as exc:
            LOG.warning("[Retry %s/%s] %s ‚Äì %s", attempt, retries, url, exc)
            if attempt == retries:
                raise
            await asyncio.sleep(backoff ** attempt)

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Sitemap discovery ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

async def _locs_from_sitemap(session: aiohttp.ClientSession, url: str) -> List[str]:
    try:
        raw = await fetch(session, url)
        if url.endswith(".gz"):
            raw = gzip.decompress(raw)
        tree = etree.parse(BytesIO(raw))
        root_name = etree.QName(tree.getroot()).localname
        return [loc.text.strip() for loc in tree.findall(".//{*}loc")]
        if root_name in {"urlset", "sitemapindex"} else []
    except Exception as exc:
        LOG.warning("Parse failed %s ‚Äì %s", url, exc)
        return []

async def discover_urls(session: aiohttp.ClientSession, sitemaps: List[str]) -> List[str]:
    queue, seen, pages = list(dict.fromkeys(sitemaps)), set(), []
    prog = st.empty()
    done = 0
    while queue:
        sm = queue.pop(0)
        if sm in seen:
            continue
        seen.add(sm)
        locs = await _locs_from_sitemap(session, sm)
        if locs and locs[0].lower().endswith((".xml", ".gz")):
            queue.extend(u for u in locs if u not in seen)  # recurse
        else:
            pages.extend(locs)
        done += 1
        if done % 5 == 0:
            prog.progress(done / (done + len(queue) + 1e-9), text="üîç Discovering URLs ‚Ä¶")
    prog.empty()
    return list(dict.fromkeys(pages))

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ GSTIN URL helpers ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def to_jamku_url(pice_url: str) -> str | None:
    if m := GSTIN_RE.search(pice_url):
        return f"https://gst.jamku.app/gstin/{m.group(0).lower()}"
    return None

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Page scan ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

async def _scan_page(
    session: aiohttp.ClientSession,
    url: str,
    patterns: List[re.Pattern],
    simple_keywords: Sequence[str],
    sem: asyncio.Semaphore,
) -> Tuple[str, str, str] | None:
    """Return (url, matched_keyword, title) or None on no‚Äëmatch; raises for HTTP errors."""
    async with sem:
        html_bytes = await fetch(session, url)
        html = html_bytes.decode("utf-8", "ignore")

        # üîé Ultra‚Äëfast pre‚Äëcheck to skip regex on obvious non‚Äëmatches
        if not any(kw in html for kw in simple_keywords):
            return None

        match = next((m for p in patterns if (m := p.search(html))), None)
        if match:
            # üåü Minimize HTML parsing ‚Äì only when match found
            soup = BeautifulSoup(html, "html.parser")
            title = (soup.title.string or "").strip() if soup.title else ""
            return url, match.group(0), title
        return None

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Jamku scan helper ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

async def _scan_jamku_wrapper(
    session: aiohttp.ClientSession,
    url: str,
    patterns: List[re.Pattern],
    simple_keywords: Sequence[str],
    sem: asyncio.Semaphore,
    rate_delay: float,
):
    async with sem:  # respect parallel limit
        try:
            res = await _scan_page(session, url, patterns, simple_keywords, sem=asyncio.Semaphore(1))
            return res
        finally:
            await asyncio.sleep(rate_delay)  # global rate‚Äëlimit

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Main crawl routine ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

async def crawl_and_retry(
    sitemaps: List[str],
    raw_keywords: List[str],
    concurrency: int,
    batch_size: int,
    jamku_rate: int,
) -> pd.DataFrame:
    """Phase‚Äë1 Piceapp scan ‚Üí parallel, rate‚Äëlimited Jamku retry."""

    tokens, root_map = build_keyword_structures(tuple(raw_keywords))  # cached
    patterns = compile_patterns(tokens)
    simple_keywords = tokens[:SAMPLE_PRECHECK]

    # HTTP client
    conn = aiohttp.TCPConnector(
        limit=concurrency,
        ttl_dns_cache=DNS_CACHE_TTL,
        force_close=False,
        enable_http2=True,
    )
    timeout = aiohttp.ClientTimeout(total=DEFAULT_TIMEOUT)
    headers = {"Accept-Encoding": "gzip"}

    results: List[Tuple[str, str, str]] = []
    failed_pice_urls: List[str] = []

    async with aiohttp.ClientSession(connector=conn, timeout=timeout, headers=headers) as session:
        # 1Ô∏è‚É£ Discover URLs
        base_pages = await discover_urls(session, sitemaps)
        st.info(f"üåê Total Piceapp pages discovered: {len(base_pages):,}")

        # 2Ô∏è‚É£ High‚Äëspeed Piceapp scan
        sem = asyncio.Semaphore(concurrency)
        scanned = 0
        prog = st.empty()
        start = time.perf_counter()

        for batch_idx, batch in enumerate(chunked(base_pages, batch_size), start=1):
            tasks = [
                _scan_page(session, u, patterns, simple_keywords, sem) for u in batch
            ]
            for res in await asyncio.gather(*tasks, return_exceptions=True):
                scanned += 1
                if isinstance(res, tuple):
                    results.append(res)
                elif isinstance(res, Exception):
                    failed_pice_urls.append(res.args[0] if res.args else "")
            # update UI sparingly
            if batch_idx % 1 == 0:
                prog.progress(
                    min(1.0, scanned / len(base_pages)),
                    text=f"‚ö° Scanning Piceapp ‚Ä¶ {scanned}/{len(base_pages):,}",
                )
            # checkpoint
            if batch_idx % CHECKPOINT_EVERY == 0:
                _checkpoint(results)
        prog.empty()
        st.success(f"‚úÖ Piceapp phase completed in {time.perf_counter() - start:.1f}s")

        # 3Ô∏è‚É£ Prepare Jamku list
        jamku_urls = [u for u in {to_jamku_url(p) for p in failed_pice_urls} if u]
        if not jamku_urls:
            st.info("üéâ No failed URLs to retry on Jamku.")
            return _to_df(results, root_map)

        st.info(f"‚ôªÔ∏è Retrying {len(jamku_urls):,} failed URLs via Jamku ({jamku_rate}/sec max)")

        # 4Ô∏è‚É£ Parallel rate‚Äëlimited Jamku scan
        sem_jamku = asyncio.Semaphore(jamku_rate)
        rate_delay = 1 / jamku_rate
        prog_j = st.empty()

        jamku_tasks = [
            _scan_jamku_wrapper(session, u, patterns, simple_keywords, sem_jamku, rate_delay)
            for u in jamku_urls
        ]
        completed = 0
        for f in asyncio.as_completed(jamku_tasks):
            res = await f
            completed += 1
            if isinstance(res, tuple):
                results.append(res)
            if completed % 10 == 0:
                prog_j.progress(completed / len(jamku_urls), text="üîÑ Scanning Jamku ‚Ä¶")
        prog_j.empty()

    return _to_df(results, root_map)

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Helpers ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def _checkpoint(records: List[Tuple[str, str, str]]):
    """Write incremental checkpoint to disk using fast ujson."""
    ts = datetime.now().strftime("%Y%m%d_%H%M%S")
    path = Path(f"checkpoint_{ts}.json")
    with path.open("w", encoding="utf-8") as fp:
        json.dump(records[-10_000:], fp)  # store latest slice to keep file size small
    LOG.info("üíæ Checkpoint saved ‚Üí %s", path)


def _to_df(records: List[Tuple[str, str, str]], root_map: Dict[str, str]) -> pd.DataFrame:
    df = pd.DataFrame(records, columns=["url", "matched_keyword", "page_title"])
    df["matched_root"] = df["matched_keyword"].map(root_map).fillna(df["matched_keyword"])
    return df[["url", "matched_keyword", "matched_root", "page_title"]]

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Streamlit UI ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

st.set_page_config(page_title="Keyword Sitemap Crawler", layout="wide")

st_autorefresh = st.experimental_rerun  # future‚Äëproof; simple keep‚Äëalive

st.title("üîç Keyword¬†Sitemap¬†Crawler¬†‚Äî Ultimate¬†Turbo üöÄ")

with st.form("crawl_form"):
    sitemaps_input = st.text_area("Sitemap URL(s) (one per line)", height=120)
    kw_file = st.file_uploader("Keyword list (TXT)", type=["txt"])
    col1, col2, col3 = st.columns(3)
    concurrency = col1.slider("Concurrency (Piceapp phase)", 20, 800, DEFAULT_CONCURRENCY, 20)
    batch_size = col2.slider("Batch size", 200, 5_000, DEFAULT_BATCH_SIZE, 200)
    jamku_rate = col3.slider("Jamku requests / sec", 1, 10, 3)
    submitted = st.form_submit_button("üöÄ Run Crawl")

if submitted:
    if not sitemaps_input or not kw_file:
        st.error("‚ùå Provide both sitemap URLs and a keyword list.")
        st.stop()

    sitemap_list = [u.strip() for u in sitemaps_input.splitlines() if u.strip()]
    raw_kw = kw_file.read().decode("utf-8", "ignore").splitlines()
    st.write(f"üìÑ {len(raw_kw):,} keywords loaded.")

    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)
    try:
        df_out = loop.run_until_complete(
            crawl_and_retry(sitemap_list, raw_kw, concurrency, batch_size, jamku_rate)
        )
    finally:
        loop.run_until_complete(loop.shutdown_asyncgens())
        loop.close()

    # ‚Äî Results
    if df_out.empty:
        st.warning("‚ö†Ô∏è No keyword matches found.")
    else:
        st.dataframe(df_out, use_container_width=True)
        csv_bytes = df_out.to_csv(index=False).encode("utf-8")
        st.download_button("üì• Download CSV", csv_bytes, "keyword_matches.csv", "text/csv")

        # Offload chart for huge outputs
        if len(df_out) < 20_000:
            chart_data = (
                df_out["matched_root"].value_counts().reset_index(names=["matched_root", "count"])
            )
            bar = (
                alt.Chart(chart_data)
                .mark_bar()
                .encode(x="matched_root:N", y="count:Q", tooltip=["matched_root", "count"])
                .properties(title="Match count by root", height=400)
            )
            st.altair_chart(bar, use_container_width=True)
        else:
            st.info("üìä Chart skipped due to large result set (‚â•20K rows).")
